# Discussion, Conclusion, and Abstract Sections

## VII. DISCUSSION

This study successfully developed and evaluated machine learning and deep learning models for detecting toxic content in Twitter posts. Through systematic experimentation, we compared a baseline TF-IDF plus Logistic Regression model with a fine-tuned BERT-base-uncased transformer model. The baseline model achieved an accuracy of 62.24% and an F1-macro score of 62.34%, demonstrating reasonable performance for a traditional machine learning approach. The fine-tuned BERT model outperformed the baseline significantly, achieving an accuracy of 65.93% and an F1-macro score of 66.17%, representing a 6.14% improvement in F1-macro score. While the BERT model did not reach the target F1-macro score of 0.85, it showed consistent superiority over the baseline approach, particularly in handling context-dependent language, sarcasm, and informal expressions commonly found in social media posts. The experiments revealed that BERT's bidirectional attention mechanism and deep contextual understanding allowed it to better distinguish between toxic and non-toxic content, especially in ambiguous cases where surface meaning contradicted true sentiment. The hyperparameter tuning process demonstrated that configurations with batch size 32 and learning rates around 2.8e-5 produced the most effective models, suggesting that larger batch sizes and moderate learning rates work well for fine-tuning BERT on social media text. The validation versus test set comparisons showed consistent performance across splits, indicating good generalization capability without significant overfitting. These findings contribute valuable insights into practical approaches for developing automated content moderation systems that can assist human moderators in managing large-scale social media platforms.

This research directly addresses Sustainable Development Goal 16 (Peace, Justice and Strong Institutions) by developing technological solutions that promote peaceful and inclusive societies through safer online communication environments. The problem of toxic content on social media platforms like Twitter creates barriers to meaningful public discourse, threatens user safety, and contributes to online harassment that disproportionately affects marginalized communities. Our project addresses this challenge by creating automated systems that can identify potentially harmful content in real-time, enabling platform administrators and content moderators to take appropriate actions more efficiently. By improving the accuracy and fairness of toxicity detection systems, this work supports the broader goal of creating digital spaces where all users can participate safely without fear of abuse or harassment. The development of interpretable and transparent models that can differentiate between genuine toxicity and harmless expressions helps prevent over-moderation while protecting vulnerable users. Furthermore, the prototype application we developed demonstrates practical usability of these models for real-world deployment, making advanced content moderation technology accessible to smaller platforms and organizations that may lack resources for extensive human moderation teams. The focus on handling sarcasm, informal language, and cultural variations in the models helps reduce bias and ensures that the systems do not unfairly target specific communities or communication styles. Through this research, we contribute to building more inclusive digital environments that support free expression while maintaining safety and respect for all users, aligning with SDG 16's targets for promoting peaceful, just, and inclusive societies both online and offline.

## VIII. CONCLUSION

This study successfully developed and evaluated both traditional machine learning and deep learning approaches for detecting toxic content in Twitter posts, demonstrating that fine-tuned BERT models significantly outperform baseline TF-IDF plus Logistic Regression models in toxicity classification tasks. While the target F1-macro score of 0.85 was not fully achieved, the BERT model's performance improvement of 6.14% over the baseline and its superior handling of context-dependent and informal language patterns provides valuable evidence for the effectiveness of transformer-based approaches in social media content moderation. The research demonstrates practical pathways for developing automated toxicity detection systems that can assist human moderators and support safer online communication environments. Future work should explore larger datasets, more sophisticated architectures, and domain-specific fine-tuning to further improve performance and reduce false positives and false negatives in toxicity detection systems.

## ABSTRACT (Maximum 300 words)

This paper explores the impact of diverse hyperparameter configurations on BERT model performance for detecting toxic content in Twitter posts. Through systematic experiments, we compared baseline TF-IDF and Logistic Regression models with fine-tuned BERT-base-uncased models on a dataset of 30,165 tweets labeled as negative (toxic), neutral, or positive. The baseline model achieved an F1-macro score of 0.6234 with 62.24% accuracy, serving as a performance benchmark for comparison. The fine-tuned BERT model significantly outperformed the baseline, reaching an F1-macro score of 0.6617 with 65.93% accuracy, representing a 6.14% improvement in F1-macro score. Our investigation focused on exploring larger batch sizes and extended training epochs, revealing that configurations with batch size 32 and learning rates around 2.8e-5 produced highly effective models. The experiments demonstrated that multiple hyperparameter combinations can achieve similar performance levels, suggesting flexibility in optimization strategies based on computational constraints. We employed automated hyperparameter tuning using both grid search and random search strategies to systematically explore different combinations of learning rates, batch sizes, training epochs, and weight decay values. The BERT models showed consistent improvement over baseline methods, providing strong evidence for the superiority of transformer-based approaches in handling context-dependent and informal language patterns found in social media posts. The models demonstrated particular strength in distinguishing between sarcastic language and genuine sentiment, which is crucial for accurate toxicity detection. We developed a lightweight prototype application consisting of a Streamlit web interface and FastAPI REST API that demonstrates practical usability of the trained models for real-world content moderation workflows. The findings offer practical guidance for developing scalable toxicity detection systems that can effectively moderate content across social media platforms while maintaining high accuracy and reliability. This research contributes to creating safer online communication environments and supports Sustainable Development Goal 16 by promoting peaceful and inclusive digital spaces through automated content moderation technology.

