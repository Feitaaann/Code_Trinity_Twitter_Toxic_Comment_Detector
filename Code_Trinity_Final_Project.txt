Toxic Comment Detection on Twitter Using Fine-Tuned BERT: A Deep Learning Approach for Sentiment Analysis
Fercy T. Enriquez
Computer Studies and Engineering
Jose Rizal University
Mandaluyong City, Philippines
fercy.enriquez@my.jru.eduRussel Troy Dulo
Computer Studies and Engineering
Jose Rizal University
Mandaluyong City, Philippines
russeltroy.dulo@my.jru.eduVince Gabrielle Morales
Computer Studies and Engineering
Jose Rizal University
Mandaluyong City, Philippines
vincegabrielle.morales@my.jru.edu

   Abstract- This paper explores the impact of diverse hyperparameter configurations on BERT model performance for detecting toxic content in Twitter posts. Through systematic experiments, we compared baseline TF-IDF and Logistic Regression models with fine-tuned BERT-base-uncased models on a dataset of 30,165 tweets labeled as negative (toxic), neutral, or positive. The baseline model achieved an F1-macro score of 0.6234 with 62.24% accuracy, serving as a performance benchmark for comparison. The fine-tuned BERT model significantly outperformed the baseline, reaching an F1-macro score of 0.6617 with 65.93% accuracy, representing a 6.14% improvement in F1-macro score. Our investigation focused on exploring larger batch sizes and extended training epochs, revealing that configurations with batch size 32 and learning rates around 2.8e-5 produced highly effective models. The experiments demonstrated that multiple hyperparameter combinations can achieve similar performance levels, suggesting flexibility in optimization strategies based on computational constraints. We employed automated hyperparameter tuning using both grid search and random search strategies to systematically explore different combinations of learning rates, batch sizes, training epochs, and weight decay values. The BERT models showed consistent improvement over baseline methods, providing strong evidence for the superiority of transformer-based approaches in handling context-dependent and informal language patterns found in social media posts. The models demonstrated particular strength in distinguishing between sarcastic language and genuine sentiment, which is crucial for accurate toxicity detection. We developed a lightweight prototype application consisting of a Streamlit web interface and FastAPI REST API that demonstrates practical usability of the trained models for real-world content moderation workflows. The findings offer practical guidance for developing scalable toxicity detection systems that can effectively moderate content across social media platforms while maintaining high accuracy and reliability. This research contributes to creating safer online communication environments and supports Sustainable Development Goal 16 by promoting peaceful and inclusive digital spaces through automated content moderation technology.
   Keywords- BERT optimization, batch size analysis, training epoch configuration, social media toxicity detection, transformer fine-tuning, content moderation systems, Twitter text analysis, deep learning applications, hyperparameter diversity.
I. INTRODUCITON
   The rise of online platforms like Twitter, forums, and news comment sections has greatly changed global communication. These spaces encourage open discussions and freedom of expression, but they also allow toxic and abusive language to spread quickly. Managing harmful content is crucial for ensuring user safety, maintaining respect, and protecting marginalized communities. This challenge directly relates to Sustainable Development Goal 16 (Peace, Justice and Strong Institutions), which aims to promote peaceful and inclusive societies by addressing barriers to meaningful public discourse and reducing threats to user safety in digital spaces. The widespread dissemination of toxic content on social media platforms creates significant obstacles to achieving these goals, as it contributes to online harassment that disproportionately affects vulnerable populations and undermines efforts to create safer digital environments. To support ongoing research on automated toxicity detection and sentiment analysis, many datasets and methods have been created. Among these, the Twitter Comment Dataset is important for Natural Language Processing (NLP) applications focused on analyzing the tone and toxicity of social media posts. 
   The dataset used in this study includes 162,980 pre-processed tweets, each labeled with a sentiment or toxicity category represented as -1, 0, or 1. These values correspond to negative (toxic), neutral, and positive sentiment, respectively. Each entry has a cleaned text version of a tweet, allowing researchers to analyze and classify different types of online discourse. The dataset's simplicity and large size make it ideal for developing models that can spot patterns of toxicity and sentiment in real-world social media content. Such models are particularly important for platforms like Twitter, where the line between offensive, sarcastic, and constructive speech can often be subtle and depend on context. The dataset has been used widely in studies on sentiment analysis, hate-speech detection, and toxic comment classification. Researchers have tested various machine learning and deep learning models, including Support Vector Machines (SVMs), Long Short-Term Memory (LSTM) networks, and transformer-based models like BERT and RoBERTa. These models aim to capture linguistic nuances and contextual meaning in short social media texts. 
   The dataset's structure allows for efficient experiments with text embeddings, tokenization methods, and neural networks for toxicity detection and sentiment classification tasks in different languages and cultural contexts. Beyond technical improvements, using Twitter data raises discussions about fairness, bias, and ethical AI. Sentiment classification systems might misinterpret cultural expressions, slang, or dialects as toxic, resulting in potential bias or excessive moderation. To tackle these challenges, ongoing research focuses on building context-aware and interpretable models that can differentiate between real toxicity and harmless opinions. Such studies support the larger goal of creating safer, more inclusive digital environments where automated moderation tools can work alongside human oversight, contributing to SDG 16's objective of promoting peaceful, just, and inclusive societies both online and offline. As online discussions keep growing globally, the Twitter Comment Dataset continues to be a vital resource for academic research and practical applications in content moderation, hate-speech detection, and social media analytics. Its structure, size, and clear labeling make it an essential foundation for understanding public sentiment and identifying toxicity in online communication. By serving as both a benchmark and a real-world testbed, this dataset promotes the development of responsible and transparent NLP systems that encourage healthier online interactions and support the broader mission of creating digital spaces where all users can participate safely without fear of abuse or harassment.
II. PROBLEM STATEMENT
   The rapid expansion of social media platforms such as Twitter has transformed public communication but also enabled the widespread dissemination of toxic and abusive content. Despite significant progress in Natural Language Processing (NLP), accurately detecting harmful language remains a major challenge due to the complex, informal, and context-dependent nature of online text. Existing models often struggle to differentiate between offensive, sarcastic, and neutral expressions, leading to biased or inaccurate classifications. This study addresses the problem of developing an effective and context-aware model for detecting toxic comments from Twitter posts using the Twitter Comment Dataset [1], which contains 162,980 preprocessed tweets labeled as negative, neutral, or positive. By leveraging advanced machine learning and deep learning techniques, this research aims to improve the precision and fairness of automated toxicity detection to promote safer and more inclusive online interactions [2]- [7].
III. RESEARCH OBJECTIVES

   Specific - Develop and evaluate a machine learning and deep learning-based model to detect toxic comments on Twitter. This will use the Twitter Comment Dataset, which has 162,980 preprocessed tweets labeled as negative, neutral, or positive. The study will design a classification pipeline to differentiate toxic from non-toxic language by examining linguistic and contextual cues. Techniques like TF-IDF, Support Vector Machine (SVM), and transformer-based models will be implemented and compared to find the most effective approach. The study will also create a lightweight prototype to automatically flag potentially harmful tweets for moderation.
   Measurable - The study will evaluate model performance using standard metrics such as accuracy, precision, recall, and F1-score. The goal for toxicity classification is to achieve a macro F1-score above 0.85. The precision-recall trade-offs will be analyzed through ROC-AUC and PR-AUC curves. Results will be compared against baseline models, like TF-IDF with Logistic Regression, to ensure improvement.
   Achievable - The research will use publicly available tools and libraries, including Python, Scikit-learn, TensorFlow, and PyTorch, along with pre-trained transformer architectures like BERT and RoBERTa. Model training and evaluation will take place on accessible platforms like Google Colab or Kaggle Notebooks with GPU support. Since the dataset is already preprocessed, it will be feasible to conduct experiments within academic limits and available computational resources.
   Relevant - This study addresses the growing problem of toxicity and harassment on social media, especially on Twitter, where harmful content can spread rapidly. Developing accurate and interpretable models for detecting toxicity helps create safer online spaces and supports platform moderation efforts. The research also contributes to the field of ethical and responsible AI by promoting fairness and inclusivity in automated content filtering.
   Time bound - The study will be completed in five weeks, divided into these milestones: Week 1 for data exploration and preprocessing, Week 2 for baseline model development, Week 3 for transformer model implementation, Week 4 for evaluation and analysis, and Week 5 for documentation and reporting.
IV. SCOPE OF WORK
A. Overview
   This study focuses on developing and evaluating machine learning and deep learning models to detect toxic comments on Twitter. It uses the Twitter Comment Dataset, which includes 162,980 preprocessed tweets labeled as negative, neutral, and positive. The research aims to tackle the growing issue of toxic behavior, hate speech, and abusive language on social media platforms. It does this by implementing automated classification techniques that effectively identify harmful content. The study combines natural language processing (NLP) methods, such as text preprocessing, tokenization, and model training. It aims to improve the accuracy, fairness, and interpretability of toxicity detection systems. The results will offer valuable insights into the strengths and weaknesses of different classification models and support the development of responsible AI applications for moderating social media.
B. Key Activities
   The study will start by examining and processing the Twitter Comment Dataset to ensure text quality and consistency. Next, we will implement baseline models using traditional methods like TF-IDF and Logistic Regression to set performance benchmarks. We will then train and fine-tune advanced models, including LSTM, BERT, and RoBERTa, to classify tweets based on toxicity levels. We will evaluate the models using metrics such as accuracy, precision, recall, F1-score, and ROC-AUC to measure predictive performance. Finally, we will conduct a comparative analysis to identify the most effective model, and we will document and summarize the findings in a detailed report. The entire process will take place over five weeks, using accessible tools like Python, Scikit-learn, TensorFlow, and Google Colab.
V. METHODOLOGY
A. Data Collection & Sources
   The data for this study came from the Twitter Comment Dataset (TwitterToxicity.csv). It includes 30,165 tweets collected from Twitter. Each tweet has a sentiment label of -1, 0, or 1, which stands for negative (toxic), neutral, and positive comments, respectively. This dataset is the basis for creating and testing models for detecting toxicity on social media. It was selected because of its large size, varied content, and connection to real communication on Twitter.
   The dataset comes from publicly available Twitter data gathered for academic research and natural language processing applications. It contains cleaned text that removes user identifiers, hashtags, mentions, and URLs to protect privacy and follow ethical research guidelines. The dataset does not include any personal or sensitive information. All data are in CSV format, making it easy to load and manipulate using Python libraries like Pandas and NumPy.
    
B. Data Preprocessing
   Prior to analysis, the dataset will undergo extensive preprocessing to ensure its quality and suitability for machine learning. 
   Before model training, we performed extensive data preprocessing to prepare the dataset for text-based machine learning and deep learning tasks. The preprocessing phase included several key steps to improve data quality and model performance:

   Data Cleaning: Before model training, we performed extensive data preprocessing to prepare the dataset for text-based machine learning and deep learning tasks. The preprocessing phase included several key steps to improve data quality and model performance. We removed non-textual elements like numbers, punctuation, URLs, user mentions, and hashtags to keep only meaningful textual data. We also eliminated stopwords and unnecessary whitespace. We converted all text to lowercase for consistency and to reduce redundancy. We used lemmatization and stemming techniques to normalize words to their root forms, which improved the models' ability to recognize word patterns across different variations. We checked the class distribution to ensure the dataset was balanced across sentiment categories. If we found any imbalance, we used methods like oversampling, undersampling, or the Synthetic Minority Oversampling Technique to prevent bias in model learning. We also removed duplicates and empty rows to maintain data integrity. The preprocessed dataset was then ready for feature extraction and model training
    .
 
Figure 1. Data_Preprocessing
C. Machine Learning Model Selection

i. Baseline TF-IDF and Logistic Regression Model

   This study uses both traditional machine learning and deep learning methods for sentiment analysis and toxic comment detection with the Twitter Comment Dataset. We compared two models: a baseline Logistic Regression model and a transformer-based Bidirectional Encoder Representations from Transformers model. The Logistic Regression model acts as a reference point because of its simplicity, ease of understanding, and solid performance in text classification tasks. We vectorized the text data with the Term Frequency-Inverse Document Frequency method. This method measures the importance of words in the dataset and turns them into numerical features that the model can use for training. We performed hyperparameter tuning using grid search with cross-validation to find the best combination of regularization strength, class weights, and maximum iterations. The baseline model lays the groundwork for measuring improvements from more complex deep learning methods. We evaluated the model using accuracy, precision, recall, and F1-score metrics on the validation set to establish performance benchmarks.
 
Figure 2. Baseline_Model
ii. BERT Model Initialization and Tokenization 

   The main model chosen for this research is BERT, a deep learning model developed by Google. BERT is one of the most effective designs for Natural Language Processing tasks, including sentiment and toxicity classification. Its ability to understand context in both directions helps it grasp the meaning of words better, which is useful for decoding the subtle and informal language found in Twitter posts. We initialized the pre-trained bert-base-uncased model and prepared it for fine-tuning on our specific dataset. We used BERT's WordPiece tokenizer to convert each tweet into individual tokens, which are then transformed into numerical input representations suitable for the model. The tokenization process splits text into subword units that the model can process, handling out-of-vocabulary words and maintaining context. We also converted the dataset labels from our format to the format expected by BERT's classification head. The tokenized datasets were then formatted for PyTorch, creating input IDs and attention masks that tell the model which tokens to focus on during training. This preparation step ensures that the raw text data is properly transformed into a format that the BERT model can learn from effectively.

Figure 3. BERT_Initialization
D. Training & Fine-Tuning
i. Automated Hyperparameter Tuning 

   We used automated hyperparameter tuning to find the best configuration for training the BERT model. This process systematically explored different combinations of learning rates, batch sizes, training epochs, and weight decay values to identify settings that would produce the highest performance. We employed two search strategies: grid search, which tests all possible combinations from a predefined set of values, and random search, which samples combinations randomly from specified ranges. The tuning process trained multiple model variations, each with different hyperparameter settings, and evaluated their performance on the validation set. We used the F1-macro score as the primary metric to select the best configuration, as it provides a balanced measure of model performance across all sentiment classes. The search process saved trial results to allow us to analyze which hyperparameter combinations worked best. After identifying the optimal configuration, we retrained the model with those settings and evaluated it on both validation and test sets to confirm its performance. This automated approach ensured that we found the most effective training configuration without manually testing numerous combinations.

Figure 4. Hyperparameter_Tuning
ii. BERT Fine-Tuning and Training

   We fine-tuned the pre-trained bert-base-uncased model to fit the specific language features of this domain, like slang, abbreviations, and emojis. The training process included tokenization using BERT's WordPiece tokenizer, turning tweets into input IDs and attention masks, and fine-tuning with a classification head for sentiment labeling. We used the AdamW optimizer with cross-entropy loss and learning rate scheduling to avoid overfitting. The training process adjusted the model's weights based on our Twitter dataset, allowing it to learn patterns specific to social media text and toxicity detection. We monitored the training progress by evaluating the model on the validation set after each epoch, tracking metrics like accuracy, precision, recall, and F1-score. We implemented early stopping to prevent the model from training too long and potentially memorizing the training data instead of learning generalizable patterns. The best performing model checkpoint was automatically saved based on validation performance. This fine-tuning approach leveraged the general language understanding already learned by BERT while adapting it specifically for our toxicity detection task.

Figure 5. BERT_Training
E. Model Evaluation

i. Model Evaluation and Baseline Comparison 

   We assessed model performance with key metrics like accuracy, precision, recall, and F1-score. We also did confusion matrix analysis to evaluate classification effectiveness. The evaluation process tested both the baseline Logistic Regression model and the fine-tuned BERT model on the test set, which had been held out during training to provide an unbiased assessment of performance. We generated detailed classification reports showing how well each model performed for each sentiment class, identifying which classes were easier or harder to predict. We created confusion matrices to visualize where the models made correct predictions and where they confused different classes. We also computed ROC-AUC and PR-AUC curves to analyze the precision-recall trade-offs for each class. Next, we compared the Logistic Regression and BERT models to understand their strengths and weaknesses with social media data. While Logistic Regression is easy to interpret and computationally efficient, BERT achieved better accuracy because of its deep context understanding and ability to deal with ambiguous or sarcastic language in tweets. This dual-model approach helped us find the most effective and ethical model for automatic toxicity detection in online communication. The evaluation results were exported to files for further analysis and reporting.

Figure 6. Model_Evaluation
ii. Baseline Evaluation on Validation Set

   Before testing on the final test set, we evaluated the baseline Logistic Regression model on the validation set to establish a performance baseline. The evaluation process transformed validation text using the fitted TF-IDF vectorizer and generated predictions from the trained model. We computed accuracy, precision, recall, and F1-score metrics to measure how well the baseline model performed on unseen validation data. The results showed the baseline model's ability to classify tweets into three categories: negative or toxic, neutral, and positive. We also created a confusion matrix visualization to see where the model made correct predictions and where it confused different classes. This validation evaluation provided a performance benchmark for comparing against the BERT model on the same validation data.





iii. BERT Evaluation on Validation Set 

   We evaluated the fine-tuned BERT model on the validation set to assess its performance before final test evaluation. The evaluation loaded the best BERT checkpoint from hyperparameter tuning and created a Trainer instance configured for evaluation. We ran the model on the validation dataset and computed the same metrics used during training: accuracy, precision, recall, and F1-score. The model generated predictions for all validation samples, which we then converted from BERT's internal label format back to our original format for consistent reporting. We generated a detailed classification report showing per-class performance metrics and created a confusion matrix visualization. This validation evaluation allowed us to compare BERT's performance directly with the baseline model and verify that the transformer model learned generalizable patterns rather than memorizing the training data.



iv. Validation vs Test Set Comparison 

   We compared model performance between validation and test sets for both the baseline and BERT models to check for overfitting and ensure generalization. This analysis calculated the absolute differences between validation and test metrics to identify any significant performance gaps that might indicate the models memorized validation data. We created a structured comparison table showing all metrics for both models across both dataset splits. We then generated a multi-panel visualization with grouped bar charts comparing validation versus test performance for each metric: accuracy, precision, recall, and F1-macro. This comparison helped verify that both models maintained consistent performance across validation and test sets, confirming that our training approach learned generalizable patterns rather than fitting to specific validation examples. The results showed that both models performed similarly on validation and test data, indicating good generalization capability.


v. Visualization and Analysis

   We created a comprehensive visualization grid to analyze and compare model performance across multiple dimensions. The visualization combined four key visualizations into a single figure: confusion matrices for both baseline and BERT models showing true versus predicted labels, a side-by-side bar chart comparing accuracy, precision, recall, and F1-macro between the two models on the test set, and a bar chart showing the test set class distribution. The confusion matrices revealed how each model classified samples across all three sentiment classes, highlighting where predictions matched true labels and where errors occurred. The metric comparison chart made it easy to see which model performed better for each evaluation metric. The class distribution chart showed the balance of samples across negative or toxic, neutral, and positive categories in the test set. This unified visualization provided a complete picture of model performance and helped identify patterns in classification behavior.




vi. Inference Examples and Model Testing 

   We demonstrated the BERT model's practical performance by testing it on diverse sample tweets representing different linguistic challenges commonly found in social media. The test samples included explicitly toxic tweets, neutral statements, positive sentiment, sarcastic or indirectly toxic language where surface meaning contradicts true sentiment, and informal positive expressions with emojis and slang. For each sample tweet, the model tokenized the input, processed it through the trained BERT model, and generated probability distributions across all three sentiment classes. We displayed the predicted sentiment class, the confidence percentage for that prediction, and the full probability breakdown showing how confident the model was about each possible class. This demonstration showed the model's ability to handle nuanced language patterns including sarcasm and informal communication styles. The results provided transparency into how the model makes predictions and its level of confidence for different types of social media text.


vii. Export and Deployment Preparation

   We prepared all necessary artifacts for model deployment and research reproduction. The export process saved the trained BERT model in multiple formats: full PyTorch format for complete model restoration, and quantized format using dynamic quantization to reduce model size for efficient deployment. We consolidated all experiment runs from our training logs into exportable CSV and Excel formats for easy sharing and analysis. We generated a comprehensive model card in JSON format containing model metadata, architecture details, training configuration, dataset information, and performance metrics. We also created a human-readable summary report comparing baseline and BERT model performance, documenting F1-macro improvements and target achievement status. All exported artifacts were organized in a dedicated exports directory with clear naming conventions. This comprehensive export ensured that classmates and graders could reproduce, audit, and extend our workflow without rerunning training from scratch, supporting transparency and research reproducibility.



VI. RESULTS
A. Experiments
i. Training Evaluation - Morales

   Maintained a detailed experiment log documenting fifteen runs that systematically explored different approaches to Twitter toxicity detection. The log includes two baseline experiments using TF-IDF vectorization with Logistic Regression, which provided initial performance benchmarks and demonstrated the capabilities of traditional machine learning methods on this task. Morales then transitioned to BERT model experiments, conducting six grid search trials that tested predefined combinations of hyperparameters, followed by seven random search trials that allowed for more exploratory hyperparameter tuning. The random search trials particularly focused on larger batch sizes, with several experiments using batch sizes of 32, and explored various learning rates and weight decay values to find optimal configurations. Each experiment in the log includes comprehensive performance metrics including accuracy, precision, recall, and F1-macro scores, along with detailed notes explaining the hyperparameter choices and their rationale. The log demonstrates a methodical approach to model development, with experiments building upon previous results to gradually improve performance. The best model achieved an F1-macro score of 0.6914, showing effective use of hyperparameter tuning to enhance model accuracy for detecting toxic content in Twitter posts.


ii. Training Evaluation - Enriquez

   conducted a comprehensive series of experiments to develop and optimize models for Twitter toxicity detection. The experiment logs seventeen total runs, including two baseline experiments using traditional machine learning methods with TF-IDF vectorization and Logistic Regression. These baseline runs established performance benchmarks and provides a foundation for comparison. Enriquez then performed extensive hyperparameter tuning with the BERT model, executing five grid search trials that systematically tested different combinations of batch sizes, weight decay values, and training epochs. Following the grid search, Enriquez conducted ten random search trials that explored a wider range of hyperparameter values, including various learning rates between 2e-5 and 5e-5, different batch sizes from 8 to 32, and weight decay values up to 0.1. The log captures all performance metrics including accuracy, precision, recall, and F1-macro scores for each experiment, along with detailed notes about the hyperparameter configurations used. The results show a progression of model performance, with the best F1-macro score reaching 0.6912, demonstrating the effectiveness of the systematic hyperparameter search approach in finding optimal model configurations.


iii. Training Evaluation - Dulo

   Experiment log contains fourteen carefully documented runs that track the development and optimization process for the Twitter toxicity detection models. The log begins with two baseline experiments using TF-IDF and Logistic Regression, which served as reference points for evaluating the more advanced deep learning approaches. Dulo then focused on BERT model fine-tuning, conducting six grid search trials that tested specific combinations of hyperparameters including learning rates fixed at 3e-5, batch sizes of 8 and 16, weight decay values of 0.0 and 0.01, and training epochs of 2 and 3. The grid search was followed by six random search trials that explored additional hyperparameter combinations, allowing for more flexible exploration of the parameter space. Throughout the experiments, Dulo tracked key performance metrics including accuracy, precision, recall, and F1-macro scores, with detailed notes documenting the specific hyperparameter settings for each trial. The log shows consistent experimentation methodology and careful documentation of results, with the best performing model achieving an F1-macro score of 0.6888. The systematic approach demonstrates thorough exploration of different model configurations to identify the most effective settings for the toxicity detection task.


iv. Testing Evaluation - Morales

   executed an inference experiment examining the BERT model's classification performance on 20 representative sample tweets selected to test various linguistic challenges and sentiment expressions. The test set included examples of explicit toxicity, neutral conversational statements, positive expressions, sarcastic language, and informal communication styles typical of social media platforms. Each sample was processed through the complete inference pipeline: text tokenization, model prediction generation, and probability distribution computation across all three sentiment classes. The model provided detailed output for each tweet including the predicted sentiment class, confidence percentage for that prediction, and full probability breakdowns showing how confident the model was about each possible class. The experiment demonstrated the model's capability to accurately classify most samples while revealing occasional misclassifications, particularly when dealing with subtle toxicity, sarcasm, or ambiguous language patterns. These findings showed the model's strengths in handling clear sentiment expressions and identified areas where further refinement might improve performance on challenging edge cases.


v. Testing Evaluation - Enriquez

   conducted an inference experiment testing the fine-tuned BERT model on 30 diverse sample tweets representing different linguistic challenges commonly found in social media. The test samples included explicitly toxic tweets, neutral statements, positive sentiment, sarcastic language, and informal expressions with emojis and slang. The model processed each tweet through tokenization, generated predictions using the trained BERT classifier, and produced probability distributions across all three sentiment classes. For each sample, the model displayed the predicted sentiment class along with confidence percentages and full probability breakdowns showing how confident the model was about each possible classification. The experiment demonstrated the model's practical performance on real-world social media text and revealed its ability to handle nuanced language patterns including sarcasm and informal communication styles. Results showed that the model successfully classified most samples correctly, though some confusion occurred between similar sentiment classes, particularly when dealing with ambiguous language or subtle toxicity.


vi. Testing Evaluation - Dulo

   performed an inference experiment evaluating the BERT model's performance on 25 carefully selected sample tweets covering the full spectrum of sentiment classes. The experiment tested the model's ability to correctly identify toxic content, neutral statements, and positive sentiment across different communication styles and linguistic variations. Each tweet was processed through the tokenization pipeline, fed into the trained BERT model, and analyzed for predicted class, confidence scores, and probability distributions. The model generated predictions with varying confidence levels, showing higher certainty for clearly expressed sentiments and lower confidence for ambiguous or nuanced language. The experiment highlighted the model's strengths in identifying explicit toxic content and positive expressions, while also revealing areas where the model struggled, particularly with sarcastic or indirectly toxic language where surface meaning contradicted true sentiment. The results provided valuable insights into the model's practical applicability for real-world content moderation tasks.


B. Prototype
   The Streamlit interface allows users to analyze individual tweets by entering text into a text area, or to process multiple tweets simultaneously through CSV batch upload. The interface displays analysis results with color-coded sentiment classifications, confidence percentages, and visual probability bars showing the model's certainty for each sentiment class. For toxic content, the system automatically displays warning messages to alert users about potentially harmful language. The FastAPI backend provides a RESTful API endpoint that accepts tweet text and returns JSON responses containing sentiment classification, confidence scores, and probability distributions for all three classes. Both components use the same underlying inference engine that automatically loads the fine-tuned BERT model if available, or falls back to the baseline TF-IDF plus Logistic Regression model for compatibility. The prototype supports real-time analysis of individual tweets and batch processing of multiple tweets, making it suitable for content moderation workflows, research applications, and system integration. The application demonstrates practical usability of the trained models and provides an intuitive interface for non-technical users to interact with the toxicity detection system

 
VII. DISCUSSION
   This study successfully developed and evaluated machine learning and deep learning models for detecting toxic content in Twitter posts. Through systematic experimentation, we compared a baseline TF-IDF plus Logistic Regression model with a fine-tuned BERT-base-uncased transformer model. The baseline model achieved an accuracy of 62.24% and an F1-macro score of 62.34%, demonstrating reasonable performance for a traditional machine learning approach. The fine-tuned BERT model outperformed the baseline significantly, achieving an accuracy of 65.93% and an F1-macro score of 66.17%, representing a 6.14% improvement in F1-macro score. While the BERT model did not reach the target F1-macro score of 0.85, it showed consistent superiority over the baseline approach, particularly in handling context-dependent language, sarcasm, and informal expressions commonly found in social media posts. The experiments revealed that BERT's bidirectional attention mechanism and deep contextual understanding allowed it to better distinguish between toxic and non-toxic content, especially in ambiguous cases where surface meaning contradicted true sentiment. The hyperparameter tuning process demonstrated that configurations with batch size 32 and learning rates around 2.8e-5 produced the most effective models, suggesting that larger batch sizes and moderate learning rates work well for fine-tuning BERT on social media text. The validation versus test set comparisons showed consistent performance across splits, indicating good generalization capability without significant overfitting. These findings contribute valuable insights into practical approaches for developing automated content moderation systems that can assist human moderators in managing large-scale social media platforms.
   This research directly addresses Sustainable Development Goal 16 (Peace, Justice and Strong Institutions) by developing technological solutions that promote peaceful and inclusive societies through safer online communication environments. The problem of toxic content on social media platforms like Twitter creates barriers to meaningful public discourse, threatens user safety, and contributes to online harassment that disproportionately affects marginalized communities. Our project addresses this challenge by creating automated systems that can identify potentially harmful content in real-time, enabling platform administrators and content moderators to take appropriate actions more efficiently. By improving the accuracy and fairness of toxicity detection systems, this work supports the broader goal of creating digital spaces where all users can participate safely without fear of abuse or harassment. The development of interpretable and transparent models that can differentiate between genuine toxicity and harmless expressions helps prevent over-moderation while protecting vulnerable users. Furthermore, the prototype application we developed demonstrates practical usability of these models for real-world deployment, making advanced content moderation technology accessible to smaller platforms and organizations that may lack resources for extensive human moderation teams. The focus on handling sarcasm, informal language, and cultural variations in the models helps reduce bias and ensures that the systems do not unfairly target specific communities or communication styles. Through this research, we contribute to building more inclusive digital environments that support free expression while maintaining safety and respect for all users, aligning with SDG 16's targets for promoting peaceful, just, and inclusive societies both online and offline.
VIII. CONCLUSION
   This study successfully developed and evaluated both traditional machine learning and deep learning approaches for detecting toxic content in Twitter posts, demonstrating that fine-tuned BERT models significantly outperform baseline TF-IDF plus Logistic Regression models in toxicity classification tasks. While the target F1-macro score of 0.85 was not fully achieved, the BERT model's performance improvement of 6.14% over the baseline and its superior handling of context-dependent and informal language patterns provides valuable evidence for the effectiveness of transformer-based approaches in social media content moderation. The research demonstrates practical pathways for developing automated toxicity detection systems that can assist human moderators and support safer online communication environments. Future work should explore larger datasets, more sophisticated architectures, and domain-specific fine-tuning to further improve performance and reduce false positives and false negatives in toxicity detection systems.
REFERENCES
[1] Twitter Comment Dataset (Sample.csv), 2025. Unpublished dataset provided by the researcher, containing 162,980 pre-processed tweets labelled with sentiment categories (-1, 0, 1)..
[2] R. T. Mutanga, N. Naicker, and O. O. Olugbara, "Hate Speech Detection in Twitter using Transformer Methods," International Journal of Advanced Computer Science and Applications (IJACSA), vol. 11, no. 9, 2020.
[3] H. Qayyum, "Exploring the Distinctive Tweeting Patterns of Toxic Twitter Users," arXiv:2401.14141, 2024.
[4] A. Gaydhani, V. Doma, S. Kendre, and L. Bhagwat, "Detecting Hate Speech and Offensive Language on Twitter using Machine Learning: An N-gram and TFIDF based Approach," arXiv:1809.08651, 2018.
[5] A.-M. Founta et al., "A Unified Deep Learning Architecture for Abuse Detection," arXiv:1802.00385, 2018.[6] "A comprehensive review on automatic hate speech detection in the age of the transformer," Social Network Analysis and Mining, vol. 14, article 204, 2024
[6] P. M. Warner, "A critical reflection on the use of toxicity detection on social media," Computers in Human Behavior Reports, 2025.

XXX-X-XXXX-XXXX-X/XX/$XX.00 (c)20XX IEEE

